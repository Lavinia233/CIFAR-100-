## 为什么要进行归一化处理？

### 归一化的目的

1. **加速收敛**：归一化后的数据分布更均匀，有助于梯度下降算法更快找到最优解。
2. **防止梯度爆炸或消失**：输入数据尺度差异过大时，可能导致梯度不稳定，归一化能缓解这一问题。
3. **提升模型泛化能力**：归一化有助于模型更好地泛化到新数据。

### 常见的归一化方法

1. **Min-Max归一化**：
   - 公式：
   - 将数据缩放到[0, 1]区间。
2. **Z-score归一化**：
   - 公式：Xnorm=X−μσ*X*norm=*σ**X*−*μ*
   - 其中，μ*μ*是均值，σ*σ*是标准差。
   - 将数据转换为均值为0、标准差为1的分布。
3. **针对图像的归一化**：
   - 图像数据通常缩放到[0, 1]或[-1, 1]。
   - 例如，CIFAR-100的像素值范围是[0, 255]，可以除以255缩放到[0, 1]。



## 三通道图像

### 三通道图像的示例

假设有一个 2x2 的三通道图像

- (255,0,0)(255,0,0) 表示纯红色。
- (0,255,0)(0,255,0) 表示纯绿色。
- (0,0,255)(0,0,255) 表示纯蓝色。
- (128,128,128)(128,128,128) 表示灰色。



## numpy.transpose()

转置，默认不设置任何参数，所有轴都转置



## plt.imshow()和plt.show()的区别

plt.imshow()显示图像，plt.show()：弹出窗口显示图像



## 定义CNN模型

```
import torch.nn as nn
import torch.nn.functional as F
```

- `torch.nn`：PyTorch 的神经网络模块，提供了构建神经网络所需的类和函数。
- `torch.nn.functional`：提供了常用的激活函数、损失函数等。

```
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(256*4*4, 512)
        self.fc2 = nn.Linear(512, 100)
```

- **卷积层**：
  - `self.conv1`：第一个卷积层，输入通道数为 3（RGB 图像），输出通道数为 64，卷积核大小为 3x3，填充为 1。
  - `self.conv2`：第二个卷积层，输入通道数为 64，输出通道数为 128，卷积核大小为 3x3，填充为 1。
  - `self.conv3`：第三个卷积层，输入通道数为 128，输出通道数为 256，卷积核大小为 3x3，填充为 1。
- **全连接层**：
  - `self.fc1`：第一个全连接层，输入大小为 `256*4*4`，输出大小为 512。
  - `self.fc2`：第二个全连接层，输入大小为 512，输出大小为 100（假设有 100 个类别）。

```
    def forward(self, x):
        x = F.relu(self.conv1(x))  # 卷积层 1 + ReLU 激活
        x = F.max_pool2d(x, 2)     # 最大池化层 1
        x = F.relu(self.conv2(x))  # 卷积层 2 + ReLU 激活
        x = F.max_pool2d(x, 2)     # 最大池化层 2
        x = F.relu(self.conv3(x))  # 卷积层 3 + ReLU 激活
        x = F.max_pool2d(x, 2)     # 最大池化层 3
        x = x.view(x.size(0), -1)  # 展平操作，将多维张量展平为一维
        x = F.relu(self.fc1(x))    # 全连接层 1 + ReLU 激活
        x = self.fc2(x)            # 全连接层 2（输出层）
        return x
     
```

- **输入**：假设输入是一个形状为 `(batch_size, 3, H, W)` 的张量，其中：
  - `batch_size` 是批次大小。
  - `3` 是输入图像的通道数（RGB 图像）。
  - `H` 和 `W` 是图像的高度和宽度。
- **网络结构**：
  1. 三个卷积层（`conv1`, `conv2`, `conv3`），每个卷积层后接 ReLU 激活函数和最大池化层。
  2. 展平操作：将卷积层的输出展平为一维向量。
  3. 两个全连接层（`fc1`, `fc2`），第一个全连接层后接 ReLU 激活函数。
- **输出**：形状为 `(batch_size, 100)` 的张量，表示每个样本属于 100 个类别的得分（logits）。





## 搜广推

搜索广告推荐系统

搜广推是互联网广告领域的核心技术，通过精准的用户画像、广告召回、排序和竞价机制，将广告与用户需求匹配，提升广告效果。它广泛应用于搜索引擎、电商平台、社交媒体等场景，依赖于机器学习、深度学习和大数据技术。尽管面临冷启动、数据稀疏性等挑战，搜广推仍然是互联网广告生态中不可或缺的一部分



## Loss.item()的功能

- 在 PyTorch 中，损失函数（如 `nn.CrossEntropyLoss()`）返回的是一个 **张量（Tensor）**，而不是一个普通的 Python 数值。
- 这个张量可能包含多个值（例如，在多 GPU 训练或批量损失计算时），但通常我们只需要一个标量值来表示当前的损失。
- `loss.item()` 的作用是从张量中提取出这个标量值，并将其转换为 Python 的浮点数（`float`）。

## 为什么在每次前向传播之前都要将梯度清零？

- 在每次反向传播之前，必须调用 `optimizer.zero_grad()` 清零梯度，以确保梯度是基于当前批次的损失计算的。
- 如果不清零梯度，梯度会累积，导致参数更新错误，模型无法正常训练。